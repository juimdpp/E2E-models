{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 21.3.1 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import re\n",
    "import time\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_objects(draw, objs, labels):\n",
    "    \"\"\"Draws the bounding box and label for each object.\"\"\"\n",
    "    for obj in objs:\n",
    "        bbox = obj.bbox\n",
    "        draw.rectangle([(bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax)], outline=\"red\", width=3)\n",
    "        draw.text(\n",
    "            (bbox.xmin + 10, bbox.ymin + 10),\n",
    "            \"%s\\n%.2f\" % (labels.get(obj.id, obj.id), obj.score),\n",
    "            fill=\"red\",\n",
    "        )\n",
    "\n",
    "\n",
    "def read_label_file(file_path):\n",
    "    \"\"\"Reads labels from a text file and returns it as a dictionary.\n",
    "    This function supports label files with the following formats:\n",
    "    + Each line contains id and description separated by colon or space.\n",
    "        Example: ``0:cat`` or ``0 cat``.\n",
    "    + Each line contains a description only. The returned label id's are based on\n",
    "        the row number.\n",
    "    Args:\n",
    "        file_path (str): path to the label file.\n",
    "    Returns:\n",
    "        Dict of (int, string) which maps label id to description.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    ret = {}\n",
    "    for row_number, content in enumerate(lines):\n",
    "        pair = re.split(r\"[:\\s]+\", content.strip(), maxsplit=1)\n",
    "        if len(pair) == 2 and pair[0].strip().isdigit():\n",
    "            ret[int(pair[0])] = pair[1].strip()\n",
    "        else:\n",
    "            ret[row_number] = content.strip()\n",
    "    return ret\n",
    "\n",
    "\n",
    "def input_details(interpreter, key):\n",
    "    \"\"\"Gets a model's input details by specified key.\n",
    "    Args:\n",
    "      interpreter: The ``tf.lite.Interpreter`` holding the model.\n",
    "      key (int): The index position of an input tensor.\n",
    "    Returns:\n",
    "      The input details.\n",
    "    \"\"\"\n",
    "    return interpreter.get_input_details()[0][key]\n",
    "\n",
    "\n",
    "def input_size(interpreter):\n",
    "    \"\"\"Gets a model's input size as (width, height) tuple.\n",
    "    Args:\n",
    "        interpreter: The ``tf.lite.Interpreter`` holding the model.\n",
    "      Returns:\n",
    "        The input tensor size as (width, height) tuple.\n",
    "    \"\"\"\n",
    "    _, height, width, _ = input_details(interpreter, \"shape\")\n",
    "    return width, height\n",
    "\n",
    "\n",
    "def input_tensor(interpreter):\n",
    "    \"\"\"Gets a model's input tensor view as numpy array of shape (height, width, 3).\n",
    "\n",
    "    Args:\n",
    "        interpreter: The ``tf.lite.Interpreter`` holding the model.\n",
    "    Returns:\n",
    "        The input tensor view as :obj:`numpy.array` (height, width, 3).\n",
    "    \"\"\"\n",
    "    tensor_index = input_details(interpreter, \"index\")\n",
    "    return interpreter.tensor(tensor_index)()[0]\n",
    "\n",
    "\n",
    "def set_resized_input(interpreter, size, resize):\n",
    "    \"\"\"Copies a resized and properly zero-padded image to a model's input tensor.\n",
    "    Args:\n",
    "        interpreter: The ``tf.lite.Interpreter`` to update.\n",
    "        size (tuple): The original image size as (width, height) tuple.\n",
    "        resize: A function that takes a (width, height) tuple, and returns an\n",
    "        image resized to those dimensions.\n",
    "    Returns:\n",
    "        The resized tensor with zero-padding as tuple\n",
    "        (resized_tensor, resize_ratio).\n",
    "    \"\"\"\n",
    "    width, height = input_size(interpreter)\n",
    "    w, h = size\n",
    "    scale = min(width / w, height / h)\n",
    "    w, h = int(w * scale), int(h * scale)\n",
    "    tensor = input_tensor(interpreter)\n",
    "    tensor.fill(0)  # padding\n",
    "    _, _, channel = tensor.shape\n",
    "    print(\"CHANNEL\", channel, (h, w, channel))\n",
    "    result = resize((w, h))\n",
    "    tensor[:h, :w] = np.reshape(result, (h, w, channel))\n",
    "    return result, (scale, scale)\n",
    "\n",
    "\n",
    "def output_tensor(interpreter, i):\n",
    "    \"\"\"Gets a model's ith output tensor.\n",
    "    Args:\n",
    "      interpreter: The ``tf.lite.Interpreter`` holding the model.\n",
    "      i (int): The index position of an output tensor.\n",
    "    Returns:\n",
    "      The output tensor at the specified position.\n",
    "    \"\"\"\n",
    "    return interpreter.tensor(interpreter.get_output_details()[i]['index'])()\n",
    "\n",
    "\n",
    "class BBox(collections.namedtuple('BBox', ['xmin', 'ymin', 'xmax', 'ymax'])):\n",
    "    \"\"\"The bounding box for a detected object.\n",
    "    .. py:attribute:: xmin\n",
    "        X-axis start point\n",
    "    .. py:attribute:: ymin\n",
    "        Y-axis start point\n",
    "    .. py:attribute:: xmax\n",
    "        X-axis end point\n",
    "    .. py:attribute:: ymax\n",
    "        Y-axis end point\n",
    "    \"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        \"\"\"The bounding box width.\"\"\"\n",
    "        return self.xmax - self.xmin\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        \"\"\"The bounding box height.\"\"\"\n",
    "        return self.ymax - self.ymin\n",
    "\n",
    "    @property\n",
    "    def area(self):\n",
    "        \"\"\"The bound box area.\"\"\"\n",
    "        return self.width * self.height\n",
    "\n",
    "    @property\n",
    "    def valid(self):\n",
    "        \"\"\"Indicates whether bounding box is valid or not (boolean).\n",
    "        A valid bounding box has xmin <= xmax and ymin <= ymax (equivalent\n",
    "        to width >= 0 and height >= 0).\n",
    "        \"\"\"\n",
    "        return self.width >= 0 and self.height >= 0\n",
    "\n",
    "    def scale(self, sx, sy):\n",
    "        \"\"\"Scales the bounding box.\n",
    "        Args:\n",
    "          sx (float): Scale factor for the x-axis.\n",
    "          sy (float): Scale factor for the y-axis.\n",
    "        Returns:\n",
    "          A :obj:`BBox` object with the rescaled dimensions.\n",
    "        \"\"\"\n",
    "        return BBox(\n",
    "            xmin=sx * self.xmin,\n",
    "            ymin=sy * self.ymin,\n",
    "            xmax=sx * self.xmax,\n",
    "            ymax=sy * self.ymax)\n",
    "\n",
    "    def translate(self, dx, dy):\n",
    "        \"\"\"Translates the bounding box position.\n",
    "        Args:\n",
    "          dx (int): Number of pixels to move the box on the x-axis.\n",
    "          dy (int): Number of pixels to move the box on the y-axis.\n",
    "        Returns:\n",
    "          A :obj:`BBox` object at the new position.\n",
    "        \"\"\"\n",
    "        return BBox(\n",
    "            xmin=dx + self.xmin,\n",
    "            ymin=dy + self.ymin,\n",
    "            xmax=dx + self.xmax,\n",
    "            ymax=dy + self.ymax)\n",
    "\n",
    "    def map(self, f):\n",
    "        \"\"\"Maps all box coordinates to a new position using a given function.\n",
    "        Args:\n",
    "          f: A function that takes a single coordinate and returns a new one.\n",
    "        Returns:\n",
    "          A :obj:`BBox` with the new coordinates.\n",
    "        \"\"\"\n",
    "        return BBox(\n",
    "            xmin=f(self.xmin),\n",
    "            ymin=f(self.ymin),\n",
    "            xmax=f(self.xmax),\n",
    "            ymax=f(self.ymax))\n",
    "\n",
    "    @staticmethod\n",
    "    def intersect(a, b):\n",
    "        \"\"\"Gets a box representing the intersection between two boxes.\n",
    "        Args:\n",
    "          a: :obj:`BBox` A.\n",
    "          b: :obj:`BBox` B.\n",
    "        Returns:\n",
    "          A :obj:`BBox` representing the area where the two boxes intersect\n",
    "          (may be an invalid box, check with :func:`valid`).\n",
    "        \"\"\"\n",
    "        return BBox(\n",
    "            xmin=max(a.xmin, b.xmin),\n",
    "            ymin=max(a.ymin, b.ymin),\n",
    "            xmax=min(a.xmax, b.xmax),\n",
    "            ymax=min(a.ymax, b.ymax))\n",
    "\n",
    "    @staticmethod\n",
    "    def union(a, b):\n",
    "        \"\"\"Gets a box representing the union of two boxes.\n",
    "        Args:\n",
    "          a: :obj:`BBox` A.\n",
    "          b: :obj:`BBox` B.\n",
    "        Returns:\n",
    "          A :obj:`BBox` representing the unified area of the two boxes\n",
    "          (always a valid box).\n",
    "        \"\"\"\n",
    "        return BBox(\n",
    "            xmin=min(a.xmin, b.xmin),\n",
    "            ymin=min(a.ymin, b.ymin),\n",
    "            xmax=max(a.xmax, b.xmax),\n",
    "            ymax=max(a.ymax, b.ymax))\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(a, b):\n",
    "        \"\"\"Gets the intersection-over-union value for two boxes.\n",
    "        Args:\n",
    "          a: :obj:`BBox` A.\n",
    "          b: :obj:`BBox` B.\n",
    "        Returns:\n",
    "          The intersection-over-union value: 1.0 meaning the two boxes are\n",
    "          perfectly aligned, 0 if not overlapping at all (invalid intersection).\n",
    "        \"\"\"\n",
    "        intersection = BBox.intersect(a, b)\n",
    "        if not intersection.valid:\n",
    "            return 0.0\n",
    "        area = intersection.area\n",
    "        return area / (a.area + b.area - area)\n",
    "\n",
    "\n",
    "\n",
    "def get_objects(interpreter,\n",
    "                score_threshold=-float('inf'),\n",
    "                image_scale=(1.0, 1.0)):\n",
    "    \"\"\"Gets results from a detection model as a list of detected objects.\n",
    "    Args:\n",
    "      interpreter: The ``tf.lite.Interpreter`` to query for results.\n",
    "      score_threshold (float): The score threshold for results. All returned\n",
    "        results have a score greater-than-or-equal-to this value.\n",
    "      image_scale (float, float): Scaling factor to apply to the bounding boxes as\n",
    "        (x-scale-factor, y-scale-factor), where each factor is from 0 to 1.0.\n",
    "    Returns:\n",
    "      A list of :obj:`Object` objects, which each contains the detected object's\n",
    "      id, score, and bounding box as :obj:`BBox`.\n",
    "    \"\"\"\n",
    "    print(interpreter.get_output_details())\n",
    "    Object = collections.namedtuple('Object', ['id', 'score', 'bbox'])\n",
    "    # If a model has signature, we use the signature output tensor names to parse\n",
    "    # the results. Otherwise, we parse the results based on some assumption of the\n",
    "    # output tensor order and size.\n",
    "    # pylint: disable=protected-access\n",
    "    signature_list = interpreter._get_full_signature_list()\n",
    "    # pylint: enable=protected-access\n",
    "    if signature_list:\n",
    "        if len(signature_list) > 1:\n",
    "            raise ValueError('Only support model with one signature.')\n",
    "        print(\"Option 1\")\n",
    "        signature = signature_list[next(iter(signature_list))]\n",
    "        count = int(interpreter.tensor(signature['outputs']['output_0'])()[0])\n",
    "        scores = interpreter.tensor(signature['outputs']['output_1'])()[0]\n",
    "        class_ids = interpreter.tensor(signature['outputs']['output_2'])()[0]\n",
    "        boxes = interpreter.tensor(signature['outputs']['output_3'])()[0]\n",
    "    elif output_tensor(interpreter, 3).size == 1:\n",
    "        print(\"Option 2\")\n",
    "        boxes = output_tensor(interpreter, 0)[0]\n",
    "        class_ids = output_tensor(interpreter, 1)[0]\n",
    "        scores = output_tensor(interpreter, 2)[0]\n",
    "        count = int(output_tensor(interpreter, 3)[0])\n",
    "    else:\n",
    "        print(\"Option 3\")\n",
    "        scores = output_tensor(interpreter, 0)[0]\n",
    "        boxes = output_tensor(interpreter, 1)[0]\n",
    "        count = (int)(output_tensor(interpreter, 2)[0])\n",
    "        class_ids = output_tensor(interpreter, 3)[0]\n",
    "\n",
    "    width, height = input_size(interpreter)\n",
    "    image_scale_x, image_scale_y = image_scale\n",
    "    sx, sy = width / image_scale_x, height / image_scale_y\n",
    "\n",
    "    def make(i):\n",
    "        ymin, xmin, ymax, xmax = boxes[i]\n",
    "        return Object(\n",
    "            id=int(class_ids[i]),\n",
    "            score=float(scores[i]),\n",
    "            bbox=BBox(xmin=xmin, ymin=ymin, xmax=xmax,\n",
    "                      ymax=ymax).scale(sx, sy).map(int))\n",
    "\n",
    "    return [make(i) for i in range(count) if scores[i] >= score_threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    model = \"ssd_mobilenet_v2_coco_quant_postprocess.tflite\"\n",
    "    input_img = \"image_0_resized_300.jpg\"\n",
    "    labels = \"coco_labels.txt\"\n",
    "    threshold = 0.5\n",
    "    output_img = \"processed.jpg\"\n",
    "    count = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    labels = read_label_file(args.labels) if args.labels else {}\n",
    "    interpreter = tf.lite.Interpreter(args.model)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    orig_image = Image.open(args.input_img)\n",
    "    w, h = orig_image.size\n",
    "    tiles = [\n",
    "        orig_image\n",
    "#         orig_image.crop((0, 0, w/2 + w/5, h/2 + h/5)), \n",
    "#              orig_image.crop((w/2 - w/5, 0, w, h/2 + h/5)),\n",
    "#              orig_image.crop((0, h/2 - h/5, w/2 + w/5, h)),\n",
    "#              orig_image.crop((w/2 - w/5, h/2 - h/5, w, h))\n",
    "            ]\n",
    "    all_objs = []\n",
    "    \n",
    "    for image in tiles:\n",
    "        _, scale = set_resized_input(\n",
    "            interpreter, image.size, lambda size: image.resize(size, Image.ANTIALIAS)\n",
    "        )\n",
    "        print(scale)\n",
    "\n",
    "        print(\"----INFERENCE TIME----\")\n",
    "        print(\n",
    "            \"Note: The first inference is slow because it includes\",\n",
    "            \"loading the model into Edge TPU memory.\",\n",
    "        )\n",
    "        for _ in range(args.count):\n",
    "            start = time.perf_counter()\n",
    "            interpreter.invoke()\n",
    "            inference_time = time.perf_counter() - start\n",
    "            objs = get_objects(interpreter, args.threshold, scale)\n",
    "            print(\"%.2f ms\" % (inference_time * 1000))\n",
    "\n",
    "        print(\"-------RESULTS--------\")\n",
    "        if not objs:\n",
    "            print(\"No objects detected\")\n",
    "\n",
    "        for obj in objs:\n",
    "            all_objs.append(obj)\n",
    "    #         print(labels.get(obj.id, obj.id))\n",
    "    #         print(\"  id:    \", obj.id)\n",
    "    #         print(\"  score: \", obj.score)\n",
    "    #         print(\"  bbox:  \", obj.bbox)\n",
    "\n",
    "    if args.output_img:\n",
    "        # image = orig_image.crop((0, 0, w/2 + w/5, h/2 + h/5)).convert(\"RGB\")\n",
    "        image = orig_image.convert(\"RGB\")\n",
    "        draw_objects(ImageDraw.Draw(image), all_objs, labels)\n",
    "        image.save(args.output_img)\n",
    "        image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANNEL 3 (300, 300, 3)\n",
      "(1.0, 1.0)\n",
      "----INFERENCE TIME----\n",
      "Note: The first inference is slow because it includes loading the model into Edge TPU memory.\n",
      "[{'name': 'TFLite_Detection_PostProcess', 'index': 259, 'shape': array([ 1, 20,  4], dtype=int32), 'shape_signature': array([ 1, 20,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 260, 'shape': array([ 1, 20], dtype=int32), 'shape_signature': array([ 1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 261, 'shape': array([ 1, 20], dtype=int32), 'shape_signature': array([ 1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 262, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Option 2\n",
      "123.18 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyunsookim/.conda/envs/research_env/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'TFLite_Detection_PostProcess', 'index': 259, 'shape': array([ 1, 20,  4], dtype=int32), 'shape_signature': array([ 1, 20,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 260, 'shape': array([ 1, 20], dtype=int32), 'shape_signature': array([ 1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 261, 'shape': array([ 1, 20], dtype=int32), 'shape_signature': array([ 1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 262, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Option 2\n",
      "115.59 ms\n",
      "[{'name': 'TFLite_Detection_PostProcess', 'index': 259, 'shape': array([ 1, 20,  4], dtype=int32), 'shape_signature': array([ 1, 20,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 260, 'shape': array([ 1, 20], dtype=int32), 'shape_signature': array([ 1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 261, 'shape': array([ 1, 20], dtype=int32), 'shape_signature': array([ 1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 262, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Option 2\n",
      "117.41 ms\n",
      "[{'name': 'TFLite_Detection_PostProcess', 'index': 259, 'shape': array([ 1, 20,  4], dtype=int32), 'shape_signature': array([ 1, 20,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 260, 'shape': array([ 1, 20], dtype=int32), 'shape_signature': array([ 1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 261, 'shape': array([ 1, 20], dtype=int32), 'shape_signature': array([ 1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 262, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Option 2\n",
      "116.64 ms\n",
      "[{'name': 'TFLite_Detection_PostProcess', 'index': 259, 'shape': array([ 1, 20,  4], dtype=int32), 'shape_signature': array([ 1, 20,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 260, 'shape': array([ 1, 20], dtype=int32), 'shape_signature': array([ 1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 261, 'shape': array([ 1, 20], dtype=int32), 'shape_signature': array([ 1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 262, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Option 2\n",
      "116.77 ms\n",
      "-------RESULTS--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/run-mailcap line 528.\n",
      "Error: no \"view\" rule for type \"image/png\" passed its test case\n",
      "       (for more information, add \"--debug=1\" on the command line)\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: www-browser: not found\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: links2: not found\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: elinks: not found\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: links: not found\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: lynx: not found\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: w3m: not found\n",
      "xdg-open: no method available for opening '/tmp/tmp_41dxa8p.PNG'\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.0 300.0\n",
      "person\n",
      "  id:     0\n",
      "  score:  0.8046875\n",
      "  bbox:   BBox(xmin=132, ymin=106, xmax=201, ymax=250)\n",
      "person\n",
      "  id:     0\n",
      "  score:  0.72265625\n",
      "  bbox:   BBox(xmin=214, ymin=86, xmax=300, ymax=227)\n",
      "backpack\n",
      "  id:     26\n",
      "  score:  0.72265625\n",
      "  bbox:   BBox(xmin=39, ymin=153, xmax=131, ymax=247)\n",
      "person\n",
      "  id:     0\n",
      "  score:  0.55859375\n",
      "  bbox:   BBox(xmin=0, ymin=87, xmax=120, ymax=252)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/run-mailcap line 528.\n",
      "Error: no \"view\" rule for type \"image/png\" passed its test case\n",
      "       (for more information, add \"--debug=1\" on the command line)\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: www-browser: not found\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: links2: not found\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: elinks: not found\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: links: not found\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: lynx: not found\n",
      "/usr/bin/xdg-open: 778: /usr/bin/xdg-open: w3m: not found\n",
      "xdg-open: no method available for opening '/tmp/tmps_pxybno.PNG'\n"
     ]
    }
   ],
   "source": [
    "display_size = (500, 500)\n",
    "def main():\n",
    "    labels = read_label_file(args.labels) if args.labels else {}\n",
    "    interpreter = tf.lite.Interpreter(args.model)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Test the model on input data.\n",
    "    orig_image = Image.open(args.input_img)\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_data = np.reshape(orig_image, (1, 300, 300, 3))\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    ###################START POSTPROCESSING##################### \n",
    "    def get_objects(interpreter,\n",
    "                score_threshold=-float('inf'),\n",
    "                image_scale=(1.0, 1.0)):\n",
    "        \"\"\"Gets results from a detection model as a list of detected objects.\n",
    "        Args:\n",
    "          interpreter: The ``tf.lite.Interpreter`` to query for results.\n",
    "          score_threshold (float): The score threshold for results. All returned\n",
    "            results have a score greater-than-or-equal-to this value.\n",
    "          image_scale (float, float): Scaling factor to apply to the bounding boxes as\n",
    "            (x-scale-factor, y-scale-factor), where each factor is from 0 to 1.0.\n",
    "        Returns:\n",
    "          A list of :obj:`Object` objects, which each contains the detected object's\n",
    "          id, score, and bounding box as :obj:`BBox`.\n",
    "        \"\"\"\n",
    "        Object = collections.namedtuple('Object', ['id', 'score', 'bbox'])\n",
    "\n",
    "        boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "        class_ids = interpreter.get_tensor(output_details[1]['index'])[0]\n",
    "        scores = interpreter.get_tensor(output_details[2]['index'])[0]\n",
    "        count = int(interpreter.get_tensor(output_details[3]['index'])[0])\n",
    "                \n",
    "        _, width, height, _ = interpreter.get_input_details()[0][\"shape\"]\n",
    "        image_scale_x, image_scale_y = image_scale\n",
    "        sx, sy = width / image_scale_x, height / image_scale_y\n",
    "        print(sx, sy)\n",
    "\n",
    "        def make(i):\n",
    "            ymin, xmin, ymax, xmax = boxes[i]\n",
    "            return Object(\n",
    "                id=int(class_ids[i]),\n",
    "                score=float(scores[i]),\n",
    "                bbox=BBox(xmin=xmin*sx, ymin=ymin*sy, xmax=xmax*sx,\n",
    "                          ymax=ymax*sy).map(int))\n",
    "\n",
    "        return [make(i) for i in range(count) if scores[i] >= score_threshold]\n",
    "\n",
    "    objs = get_objects(interpreter, args.threshold)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###################END POSTPROCESSING#####################\n",
    "    all_objs = []\n",
    "    for obj in objs:\n",
    "        all_objs.append(obj)\n",
    "        print(labels.get(obj.id, obj.id))\n",
    "        print(\"  id:    \", obj.id)\n",
    "        print(\"  score: \", obj.score)\n",
    "        print(\"  bbox:  \", obj.bbox)\n",
    "\n",
    "    if args.output_img:\n",
    "        # image = orig_image.crop((0, 0, w/2 + w/5, h/2 + h/5)).convert(\"RGB\")\n",
    "        orig_image = orig_image.resize((500, 500))\n",
    "        image = orig_image.convert(\"RGB\")\n",
    "        draw_objects(ImageDraw.Draw(image), all_objs, labels)\n",
    "        image.save(args.output_img)\n",
    "        image.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "research_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
